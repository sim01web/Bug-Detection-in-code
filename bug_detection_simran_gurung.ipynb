{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2939c363-d481-49e1-8451-e65675c9b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,Dropout,Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "700af9a3-3986-4ccc-b86c-bc60f1e6bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inferred_bugs(dataset_path):\n",
    "    bug_data = []\n",
    "    for repo in os.listdir(dataset_path):\n",
    "        repo_path = os.path.join(dataset_path, repo)\n",
    "        if os.path.isdir(repo_path):\n",
    "            for bug_id in os.listdir(repo_path):\n",
    "                bug_path = os.path.join(repo_path, bug_id)\n",
    "                bug_json_path = os.path.join(bug_path, 'bug.json')\n",
    "                method_before_path = os.path.join(bug_path, 'method_before.txt')\n",
    "\n",
    "                if os.path.exists(bug_json_path,) and os.path.exists(method_before_path):\n",
    "                    with open(bug_json_path, 'r',encoding='utf-8') as bug_file, open(method_before_path, 'r',encoding='utf-8') as method_file:\n",
    "                        bug_info = json.load(bug_file)\n",
    "                        method_before = method_file.read()\n",
    "                        bug_type = bug_info.get(\"bug_type\",\"unknown\")\n",
    "                        bug_class = bug_info.get(\"bug_class\",\"unknown\")\n",
    "                        kind = bug_info.get(\"kind\",\"unknown\")\n",
    "                        visibility = bug_info.get(\"visibility\",\"unknown\")\n",
    "                        severity = bug_info.get(\"severity\",\"unknown\")\n",
    "                        bug_data.append({'bug_class': bug_class, 'kind':kind,'visibility':visibility, 'severity': severity, 'method_before': method_before, 'bug_type':bug_type})\n",
    "    return bug_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "466be9a3-d7bc-44ea-bcf5-c940ee978b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"C:\\Users\\simra\\Downloads\\data_bug_detection\"\n",
    "bug_data = load_inferred_bugs(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6188a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to csv\n",
    "def convert_json_to_csv_with_pandas(bug_data, csv_file_path):\n",
    "    df = pd.DataFrame(bug_data)\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Example usage\n",
    "convert_json_to_csv_with_pandas(bug_data, \"output_bugs_pandas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c0df779a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bug_class</th>\n",
       "      <th>kind</th>\n",
       "      <th>visibility</th>\n",
       "      <th>severity</th>\n",
       "      <th>method_before</th>\n",
       "      <th>bug_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PROVER</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>user</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>public PreparedStatement prepareUpdateFields(...</td>\n",
       "      <td>NULL_DEREFERENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PROVER</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>user</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>public Map&lt;CQLQueryType, PreparedStatement&gt; p...</td>\n",
       "      <td>NULL_DEREFERENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PROVER</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>user</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>public boolean initForceBatchStatementsOrd...</td>\n",
       "      <td>NULL_DEREFERENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PROVER</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>user</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>public void addInterceptorsToEntityMetas(L...</td>\n",
       "      <td>NULL_DEREFERENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PROVER</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>user</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>private TypeParsingResult parseComputedTyp...</td>\n",
       "      <td>NULL_DEREFERENCE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  bug_class   kind visibility severity  \\\n",
       "0    PROVER  ERROR       user     HIGH   \n",
       "1    PROVER  ERROR       user     HIGH   \n",
       "2    PROVER  ERROR       user     HIGH   \n",
       "3    PROVER  ERROR       user     HIGH   \n",
       "4    PROVER  ERROR       user     HIGH   \n",
       "\n",
       "                                       method_before          bug_type  \n",
       "0   public PreparedStatement prepareUpdateFields(...  NULL_DEREFERENCE  \n",
       "1   public Map<CQLQueryType, PreparedStatement> p...  NULL_DEREFERENCE  \n",
       "2      public boolean initForceBatchStatementsOrd...  NULL_DEREFERENCE  \n",
       "3      public void addInterceptorsToEntityMetas(L...  NULL_DEREFERENCE  \n",
       "4      private TypeParsingResult parseComputedTyp...  NULL_DEREFERENCE  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\simra\\Downloads\\output_bugs_pandas.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "68176cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bug_type:  ['NULL_DEREFERENCE' 'RESOURCE_LEAK' 'THREAD_SAFETY_VIOLATION'\n",
      " 'CHECKERS_IMMUTABLE_CAST' 'CHECKERS_PRINTF_ARGS'\n",
      " 'INTERFACE_NOT_THREAD_SAFE' 'UNSAFE_GUARDED_BY_ACCESS']\n",
      "bug_class:  ['PROVER']\n",
      "bug_kind_values:  ['ERROR']\n",
      "visibility_values:  ['user']\n",
      "severity_values:  ['HIGH']\n"
     ]
    }
   ],
   "source": [
    "target_labels = df[\"bug_type\"].unique()\n",
    "print(\"bug_type: \",target_labels)\n",
    "bug_class_values = df[\"bug_class\"].unique()\n",
    "print(\"bug_class: \",bug_class_values)\n",
    "bug_kind_values = df[\"kind\"].unique()\n",
    "print(\"bug_kind_values: \",bug_kind_values)\n",
    "visibility_values = df[\"visibility\"].unique()\n",
    "print(\"visibility_values: \",visibility_values)\n",
    "severity_values = df[\"severity\"].unique()\n",
    "print(\"severity_values: \",severity_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "236d3248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3003, 6)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c2dde9",
   "metadata": {},
   "source": [
    "##### We see that the columns 'bug_class', 'kind', 'visibility', 'severity' have only one type of value, so they do not effect our target column \"bug_type\". Hence we can discard them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c9c57f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bug_type\n",
       "NULL_DEREFERENCE             1216\n",
       "RESOURCE_LEAK                1197\n",
       "THREAD_SAFETY_VIOLATION       497\n",
       "CHECKERS_PRINTF_ARGS           43\n",
       "INTERFACE_NOT_THREAD_SAFE      36\n",
       "CHECKERS_IMMUTABLE_CAST         9\n",
       "UNSAFE_GUARDED_BY_ACCESS        5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bug_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd899de",
   "metadata": {},
   "source": [
    "##### Here, the majority of the samlples are grouped into 3 major bug types namely 'NULL_DEREFERENCE', 'RESOURCE_LEAK' and 'THREAD_SAFETY_VIOLATION'. Apart from these three types, we discard the other types as they contain very less number of samples in comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8c86eefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bug_type\n",
       "NULL_DEREFERENCE           1216\n",
       "RESOURCE_LEAK              1197\n",
       "THREAD_SAFETY_VIOLATION     497\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[(df['bug_type'] == 'NULL_DEREFERENCE') | \n",
    "            (df['bug_type'] == 'RESOURCE_LEAK') | \n",
    "            (df['bug_type'] == 'THREAD_SAFETY_VIOLATION')]\n",
    "df['bug_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e1cc7a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NULL_DEREFERENCE', 'RESOURCE_LEAK', 'THREAD_SAFETY_VIOLATION'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bug_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9bd1d5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2910, 6)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "10435261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming 'bug_type' has 3 categories (for example: ['NULL_DEREFERENCE', 'RESOURCE_LEAK', 'THREAD_SAFETY_VIOLATION'])\n",
    "dict1 = {'NULL_DEREFERENCE':0, 'RESOURCE_LEAK':1, 'THREAD_SAFETY_VIOLATION':2}\n",
    "df['bug_type'] = df['bug_type'].map(dict1)\n",
    "# Convert the integer-encoded labels to one-hot encoded vectors\n",
    "#one_hot_labels = to_categorical(encoded_labels, num_classes=num_classes)\n",
    "df['bug_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a1aaadae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bug_class</th>\n",
       "      <th>kind</th>\n",
       "      <th>visibility</th>\n",
       "      <th>severity</th>\n",
       "      <th>method_before</th>\n",
       "      <th>bug_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PROVER</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>user</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>public PreparedStatement prepareUpdateFields(...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PROVER</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>user</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>public Map&lt;CQLQueryType, PreparedStatement&gt; p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PROVER</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>user</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>public boolean initForceBatchStatementsOrd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PROVER</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>user</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>public void addInterceptorsToEntityMetas(L...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PROVER</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>user</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>private TypeParsingResult parseComputedTyp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  bug_class   kind visibility severity  \\\n",
       "0    PROVER  ERROR       user     HIGH   \n",
       "1    PROVER  ERROR       user     HIGH   \n",
       "2    PROVER  ERROR       user     HIGH   \n",
       "3    PROVER  ERROR       user     HIGH   \n",
       "4    PROVER  ERROR       user     HIGH   \n",
       "\n",
       "                                       method_before  bug_type  \n",
       "0   public PreparedStatement prepareUpdateFields(...         0  \n",
       "1   public Map<CQLQueryType, PreparedStatement> p...         0  \n",
       "2      public boolean initForceBatchStatementsOrd...         0  \n",
       "3      public void addInterceptorsToEntityMetas(L...         0  \n",
       "4      private TypeParsingResult parseComputedTyp...         0  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3dc37fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = to_categorical(df['bug_type'], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c03e986d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of sequences in the dataset: 8288\n"
     ]
    }
   ],
   "source": [
    "#Tokenization and Padding\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the text data in 'method_before' column\n",
    "tokenizer.fit_on_texts(df['method_before'])\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(df['method_before'])\n",
    "sequence_lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "# Find the maximum length\n",
    "max_length = max(sequence_lengths)\n",
    "\n",
    "print(\"Maximum length of sequences in the dataset:\", max_length)\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_length = 10000  # adjusted this based on my dataset\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "20fbc1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings from a file\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Path to the GloVe file\n",
    "glove_file_path = r\"C:\\Users\\simra\\Desktop\\MISC COURSES\\ABSA\\glove.6B.200d.txt\"\n",
    "embeddings_index = load_glove_embeddings(glove_file_path)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Initialize the embedding matrix\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_dim = 200  # The dimension of GloVe vectors\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Populate the embedding matrix with GloVe vectors\n",
    "for word, i in word_index.items():\n",
    "    if word in embeddings_index:\n",
    "        embedding_matrix[i] = embeddings_index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0d517636",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df[\"bug_type\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "49b54c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m447s\u001b[0m 6s/step - accuracy: 0.6018 - loss: 0.8614 - val_accuracy: 0.8436 - val_loss: 0.4159\n",
      "Epoch 2/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m450s\u001b[0m 6s/step - accuracy: 0.9337 - loss: 0.2303 - val_accuracy: 0.8797 - val_loss: 0.3326\n",
      "Epoch 3/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 6s/step - accuracy: 0.9868 - loss: 0.0747 - val_accuracy: 0.8900 - val_loss: 0.3199\n",
      "Epoch 4/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 6s/step - accuracy: 0.9886 - loss: 0.0469 - val_accuracy: 0.8814 - val_loss: 0.3515\n",
      "Epoch 5/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 6s/step - accuracy: 0.9928 - loss: 0.0512 - val_accuracy: 0.9107 - val_loss: 0.3053\n",
      "Epoch 6/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m463s\u001b[0m 6s/step - accuracy: 0.9902 - loss: 0.1153 - val_accuracy: 0.9089 - val_loss: 0.2972\n",
      "Epoch 7/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 6s/step - accuracy: 0.9871 - loss: 0.0621 - val_accuracy: 0.8935 - val_loss: 0.3374\n",
      "Epoch 8/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 6s/step - accuracy: 0.9914 - loss: 0.0423 - val_accuracy: 0.9089 - val_loss: 0.3267\n",
      "Epoch 9/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 6s/step - accuracy: 0.9926 - loss: 0.0520 - val_accuracy: 0.9038 - val_loss: 0.3239\n",
      "Epoch 10/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 6s/step - accuracy: 0.9925 - loss: 0.0379 - val_accuracy: 0.9141 - val_loss: 0.3041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21983934970>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Embedding, Dense\n",
    "\n",
    "# Define a simple CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_shape=(max_length,),\n",
    "                embeddings_initializer=Constant(embedding_matrix),trainable=False,))\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train CNN model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32,validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ff62181b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 268ms/step - accuracy: 0.9153 - loss: 0.2591\n",
      "cnn_accuracy with glove:  0.9140893220901489\n"
     ]
    }
   ],
   "source": [
    "cnn_accuracy_gl = model.evaluate(X_test, y_test)[1]\n",
    "print(\"cnn_accuracy with glove: \",cnn_accuracy_gl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "61947d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 280ms/step - accuracy: 0.9373 - loss: 0.1833\n",
      "cnn_accuracy:  0.9278350472450256\n"
     ]
    }
   ],
   "source": [
    "cnn_accuracy = model.evaluate(X_test, y_test)[1]\n",
    "print(\"cnn_accuracy: \",cnn_accuracy) #without glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ae1e37ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "#Saving the CNN Model trained without GloVe\n",
    "model.save('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "50c6989a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = load_model('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d0cfcf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "NULL_DEREFERENCE\n"
     ]
    }
   ],
   "source": [
    "code_snippet = \"\"\" public void printState (PrintStream out) {\n",
    "\t\tAlphabet a = instances.getDataAlphabet();\n",
    "\t\tout.println (\"#doc pos typeindex type topic\");\n",
    "\t\tfor (int di = 0; di < topics.length; di++) {\n",
    "\t\t\tFeatureSequence fs = (FeatureSequence) instances.get(di).getData();\n",
    "\t\t\tfor (int token = 0; token < topics[di].length; token++) {\n",
    "\t\t\t\tint type = fs.getIndexAtPosition(token);\n",
    "\t\t\t\tout.print(di); out.print(' ');\n",
    "\t\t\t\tout.print(token); out.print(' ');\n",
    "\t\t\t\tout.print(type); out.print(' ');\n",
    "\t\t\t\tout.print(a.lookupObject(type)); out.print(' ');\n",
    "\t\t\t\tout.print(topics[di][token]); out.println();\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t}\n",
    "\"\"\"\n",
    "# Tokenize and pad the new code snippet\n",
    "sequence = tokenizer.texts_to_sequences([code_snippet])\n",
    "padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')   \n",
    "# Get the model's prediction (returns probabilities for each class)\n",
    "prediction = model.predict(padded_sequence) \n",
    "# Get the class with the highest probability\n",
    "predicted_class = np.argmax(prediction, axis=-1)[0]\n",
    "if predicted_class == 0:\n",
    "    print(\"NULL_DEREFERENCE\")\n",
    "elif predicted_class == 1:\n",
    "    print(\"RESOURCE_LEAK\")\n",
    "elif predicted_class == 2:\n",
    "    print(\"THREAD_SAFETY_VIOLATION\")\n",
    "#print(predicted_class) \n",
    "# Convert the class index back to the original label\n",
    "#bug_type = label_encoder.inverse_transform([predicted_class])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "24f2498b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simra\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1029s\u001b[0m 14s/step - accuracy: 0.4163 - loss: 1.0468 - val_accuracy: 0.4107 - val_loss: 1.0473\n",
      "Epoch 2/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1035s\u001b[0m 14s/step - accuracy: 0.4027 - loss: 1.0369 - val_accuracy: 0.3986 - val_loss: 1.0475\n",
      "Epoch 3/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1037s\u001b[0m 14s/step - accuracy: 0.4156 - loss: 1.0452 - val_accuracy: 0.3986 - val_loss: 1.0510\n",
      "Epoch 4/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1033s\u001b[0m 14s/step - accuracy: 0.4170 - loss: 1.0557 - val_accuracy: 0.3986 - val_loss: 1.0575\n",
      "Epoch 5/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m997s\u001b[0m 14s/step - accuracy: 0.4090 - loss: 1.0387 - val_accuracy: 0.3986 - val_loss: 1.0483\n",
      "Epoch 6/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1041s\u001b[0m 14s/step - accuracy: 0.4060 - loss: 1.0331 - val_accuracy: 0.3986 - val_loss: 1.0554\n",
      "Epoch 7/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1127s\u001b[0m 15s/step - accuracy: 0.4244 - loss: 1.0223 - val_accuracy: 0.3986 - val_loss: 1.0494\n",
      "Epoch 8/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1136s\u001b[0m 16s/step - accuracy: 0.4016 - loss: 1.0341 - val_accuracy: 0.3986 - val_loss: 1.0489\n",
      "Epoch 9/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1123s\u001b[0m 15s/step - accuracy: 0.4236 - loss: 1.0361 - val_accuracy: 0.4107 - val_loss: 1.0486\n",
      "Epoch 10/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1096s\u001b[0m 15s/step - accuracy: 0.4165 - loss: 1.0377 - val_accuracy: 0.4107 - val_loss: 1.0516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2195ab3fca0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 3\n",
    "# Build and compile the model\n",
    "input_length = padded_sequences.shape[1]\n",
    "# Build the LSTM model\n",
    "def build_lstm_model(vocab_size, embedding_dim, embedding_matrix, input_length, num_classes):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = Embedding(input_dim=vocab_size, \n",
    "                        output_dim=embedding_dim,  \n",
    "                        input_shape=(max_length,),\n",
    "                        embeddings_initializer=Constant(embedding_matrix), \n",
    "                        trainable=False)  # Freezing the GloVe embeddings\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    # LSTM layer\n",
    "    model.add(LSTM(128, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Dense output layer for multi-class classification\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # Use 'softmax' for multi-class\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='categorical_crossentropy',  # Use categorical crossentropy for multi-class\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_lstm_model(vocab_size, embedding_dim, embedding_matrix, input_length, num_classes)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, one_hot_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "415ac858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 5s/step - accuracy: 0.3984 - loss: 1.0481\n",
      "0.41065293550491333\n"
     ]
    }
   ],
   "source": [
    "glove_accuracy = model.evaluate(X_test, y_test)[1]\n",
    "print(glove_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0981dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict bug type for a new code snippet\n",
    "def predict_bug_type(model, tokenizer, code_snippet, max_length, label_encoder):\n",
    "    # Tokenize and pad the new code snippet\n",
    "    sequence = tokenizer.texts_to_sequences([code_snippet])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # Get the model's prediction (returns probabilities for each class)\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    \n",
    "    # Get the class with the highest probability\n",
    "    predicted_class = np.argmax(prediction, axis=-1)[0]\n",
    "    \n",
    "    # Convert the class index back to the original label\n",
    "    bug_type = label_encoder.inverse_transform([predicted_class])\n",
    "    \n",
    "    return bug_type[0]\n",
    "\n",
    "# Example usage\n",
    "new_code = \"public void someFunction() { // code }\"\n",
    "predicted_bug_type = predict_bug_type(model, tokenizer, new_code, max_length, label_encoder)\n",
    "print(\"Predicted Bug Type:\", predicted_bug_type)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
